from airflow import DAG
from airflow.decorators import task
from datetime import datetime
import os

# é€™è£¡å»ºè­°ä½¿ç”¨ä½ ä¹‹å‰çš„ S3 è®Šé‡
S3_BUCKET_NAME = "data-platform-university-labs" # æ”¹æˆä½ çš„æ¡¶å
S3_CONN_ID = "aws_s3_conn"

default_args = {
	'owner': 'airflow',
	'start_date': datetime(2023, 1, 1),
}

@dag(
		default_args=default_args,
		schedule_interval=None,
		catchup=False,
		tags=['spark', 'iceberg', 'test']
    )
def dag_spark_iceberg_validation():

	@task
	def test_pyspark_iceberg(ds=None):
		from pyspark.sql import SparkSession
		import os
		from airflow.providers.amazon.aws.hooks.base_aws import AwsBaseHook

	aws_hook = AwsBaseHook(aws_conn_id="aws_s3_conn", client_type="s3")
credentials = aws_hook.get_credentials()
	aws_access_key = credentials.access_key
	aws_secret_key = credentials.secret_key
# å¦‚æœä½ çš„è¿æ¥é‡Œé…ç½®äº† tokenï¼ˆä¸´æ—¶å‡­è¯ï¼‰ï¼Œä¹Ÿæ‹¿å‡ºæ¥
#aws_session_token = credentials.token
	print("ğŸš€ æ­£åœ¨åˆå§‹åŒ– SparkSession (å¸¶ç€ Iceberg é…ä»¶)...")

# é—œéµé…ç½®ï¼šé€™æ±ºå®šäº† Spark èƒ½ä¸èƒ½ç©è½‰ Iceberg
	spark = SparkSession.builder \
		.appName("SparkIcebergTest") \
		.config("spark.jars.packages", 
				"org.apache.iceberg:iceberg-spark-runtime-3.4_2.12:1.3.1,"
				"org.apache.hadoop:hadoop-aws:3.3.4") \
			.config("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions") \
			.config("spark.sql.catalog.local", "org.apache.iceberg.spark.SparkCatalog") \
			.config("spark.sql.catalog.local.type", "hadoop") \
			.config("spark.sql.catalog.local.warehouse", f"s3a://data-platform-university-labs/iceberg-warehouse") \
# --- AWS S3 ä¸“å±é…ç½® ---
			.config("spark.hadoop.fs.s3a.access.key", aws_access_key) \
				.config("spark.hadoop.fs.s3a.secret.key", aws_secret_key) \
				.config("spark.hadoop.fs.s3a.endpoint.region", "us-east-1") \
				.config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
# å¯ç”¨ AWS SDK é»˜è®¤çš„å‡­è¯æä¾›è€…éˆï¼ˆå¯é€‰ï¼Œå¦‚æœä½ æƒ³ç”¨ IAM Roleï¼‰
				.config("spark.hadoop.fs.s3a.aws.credentials.provider", "org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider") \
					.getOrCreate()

					print("âœ… SparkSession åˆå§‹åŒ–æˆåŠŸï¼")

# 1. æ¸¬è©¦ï¼šå‰µå»ºä¸€å€‹ç°¡å–®çš„ DataFrame ä¸¦å¯«å…¥ Iceberg è¡¨
					data = [("China", 398), ("USA", 500), ("Japan", 200)]
	columns = ["country", "university_count"]
test_df = spark.createDataFrame(data, columns)

	print("ğŸ“ æ­£åœ¨å˜—è©¦å¯«å…¥ Iceberg è¡¨...")
# åœ¨ local catalog ä¸‹å‰µå»ºä¸€å€‹åç‚º test_table çš„è¡¨
	test_df.writeTo("local.db.test_iceberg_table") \
		.tableProperty("format-version", "2") \
		.createOrReplace()

		print("ğŸ‰ Iceberg è¡¨å¯«å…¥æˆåŠŸï¼")

# 2. æ¸¬è©¦ï¼šè®€å–å‰›æ‰å¯«å…¥çš„è¡¨
	read_df = spark.table("local.db.test_iceberg_table")
read_df.show()

	print(f"ğŸ“ˆ è®€å–æˆåŠŸï¼Œç¸½è¡Œæ•¸: {read_df.count()}")

spark.stop()

test_pyspark_iceberg()

dag_spark_iceberg_validation_instance = dag_spark_iceberg_validation()
