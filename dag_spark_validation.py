from airflow.decorators import dag, task
from datetime import datetime
import os

# é€™è£¡å»ºè­°ä½¿ç”¨ä½ ä¹‹å‰çš„ S3 è®Šé‡
S3_BUCKET_NAME = "data-platform-university-labs"
S3_CONN_ID = "aws_s3_conn"

default_args = {
    'owner': 'airflow',
    'start_date': datetime(2023, 1, 1),
}

@dag(
    default_args=default_args,
    schedule_interval=None,
    catchup=False,
    tags=['spark', 'iceberg', 'test']
)
def dag_spark_iceberg_validation():

    @task
    def test_pyspark_iceberg(ds=None):
        from pyspark.sql import SparkSession
        from airflow.providers.amazon.aws.hooks.base_aws import AwsBaseHook
        import os

        # é€™è£¡çš„ä»£ç¢¼å¿…é ˆæ¯” def ç¸®é€²æ›´å¤š
        aws_hook = AwsBaseHook(aws_conn_id="aws_s3_conn", client_type="s3")
        credentials = aws_hook.get_credentials()
        aws_access_key = credentials.access_key
        aws_secret_key = credentials.secret_key
        
        print("ğŸš€ æ­£åœ¨åˆå§‹åŒ– SparkSession (å¸¶ç€ Iceberg é…ä»¶)...")

        # é—œéµé…ç½®
        spark = SparkSession.builder \
            .appName("SparkIcebergTest") \
            .config("spark.jars.packages",
                    "org.apache.iceberg:iceberg-spark-runtime-3.4_2.12:1.3.1,"
                    "org.apache.hadoop:hadoop-aws:3.3.4") \
            .config("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions") \
            .config("spark.sql.catalog.local", "org.apache.iceberg.spark.SparkCatalog") \
            .config("spark.sql.catalog.local.type", "hadoop") \
            .config("spark.sql.catalog.local.warehouse", f"s3a://{S3_BUCKET_NAME}/iceberg-warehouse") \
            .config("spark.hadoop.fs.s3a.access.key", aws_access_key) \
            .config("spark.hadoop.fs.s3a.secret.key", aws_secret_key) \
            .config("spark.hadoop.fs.s3a.endpoint.region", "us-east-1") \
            .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
            .config("spark.hadoop.fs.s3a.aws.credentials.provider", "org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider") \
            .getOrCreate()

        print("âœ… SparkSession åˆå§‹åŒ–æˆåŠŸï¼")

        # 1. æ¸¬è©¦ï¼šå¯«å…¥
        data = [("China", 398), ("USA", 500), ("Japan", 200)]
        columns = ["country", "university_count"]
        test_df = spark.createDataFrame(data, columns)

        print("ğŸ“ æ­£åœ¨å˜—è©¦å¯«å…¥ Iceberg è¡¨...")
        test_df.writeTo("local.db.test_iceberg_table") \
            .tableProperty("format-version", "2") \
            .createOrReplace()

        print("ğŸ‰ Iceberg è¡¨å¯«å…¥æˆåŠŸï¼")

        # 2. æ¸¬è©¦ï¼šè®€å–
        read_df = spark.table("local.db.test_iceberg_table")
        read_df.show()
        print(f"ğŸ“ˆ è®€å–æˆåŠŸï¼Œç¸½è¡Œæ•¸: {read_df.count()}")

        spark.stop()

    # é€™è£¡èª¿ç”¨ task
    test_pyspark_iceberg()

# å¯¦ä¾‹åŒ– DAG
dag_spark_iceberg_validation_instance = dag_spark_iceberg_validation()
