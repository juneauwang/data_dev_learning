from airflow.decorators import dag, task
from datetime import datetime
import json

# é…ç½®å¸¸é‡
S3_BUCKET = "data-platform-university-labs"
S3_CONN_ID = "aws_s3_conn"
BRONZE_KEY = "bronze/universities/all_universities_raw.json"

default_args = {
    'owner': 'airflow',
    'start_date': datetime(2023, 1, 1),
}

@dag(
    default_args=default_args,
    schedule_interval=None,  # æ‰‹åŠ¨è§¦å‘
    catchup=False,
    tags=['full_load', 'iceberg', 'silver']
)
def university_full_load_pipeline():

    @task
    def task_bronze_ingest():
        """ç¬¬ä¸€æ­¥ï¼šå…¨é‡æ‹‰å– API æ•°æ®å¹¶å­˜å…¥ S3 (Bronze å±‚)"""
        import requests
        from airflow.providers.amazon.aws.hooks.s3 import S3Hook

        print("ğŸ“¡ æ­£åœ¨ä» API è·å–å…¨é‡å¤§å­¦æ•°æ®...")
        url = "http://universities.hipolabs.com/search"
        response = requests.get(url)
        data = response.json()
        print(f"âœ… è·å–æˆåŠŸï¼Œå…± {len(data)} æ¡è®°å½•")

        # å°†åŸå§‹ JSON å­˜å…¥ S3
        s3_hook = S3Hook(aws_conn_id=S3_CONN_ID)
        s3_hook.load_string(
            string_data=json.dumps(data),
            key=BRONZE_KEY,
            bucket_name=S3_BUCKET,
            replace=True
        )
        print(f"ğŸ“¦ åŸå§‹æ•°æ®å·²å­˜å…¥: s3://{S3_BUCKET}/{BRONZE_KEY}")
        return BRONZE_KEY

    @task
    def task_silver_spark_transform(bronze_key):
        """ç¬¬äºŒæ­¥ï¼šä½¿ç”¨ Spark è¯»å– Bronze JSON å¹¶å†™å…¥ Iceberg (Silver å±‚)"""
        from pyspark.sql import SparkSession
        from pyspark.sql.functions import col, array_join
        from airflow.providers.amazon.aws.hooks.base_aws import AwsBaseHook

        # 1. åŠ¨æ€è·å– AWS å‡­è¯
        aws_hook = AwsBaseHook(aws_conn_id=S3_CONN_ID, client_type="s3")
        creds = aws_hook.get_credentials()

        # 2. åˆå§‹åŒ– Spark
        print("ğŸš€ å¯åŠ¨ Spark å¼•æ“...")
        spark = SparkSession.builder \
            .appName("BronzeToSilverFullLoad") \
            .config("spark.jars.packages", 
                    "org.apache.iceberg:iceberg-spark-runtime-3.4_2.12:1.3.1,"
                    "org.apache.hadoop:hadoop-aws:3.3.4") \
            .config("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions") \
            .config("spark.sql.catalog.local", "org.apache.iceberg.spark.SparkCatalog") \
            .config("spark.sql.catalog.local.type", "hadoop") \
            .config("spark.sql.catalog.local.warehouse", f"s3a://{S3_BUCKET}/iceberg-warehouse") \
            .config("spark.hadoop.fs.s3a.access.key", creds.access_key) \
            .config("spark.hadoop.fs.s3a.secret.key", creds.secret_key) \
            .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
            .config("spark.hadoop.fs.s3a.endpoint.region", "us-east-1") \
            .getOrCreate()

        # 3. è¯»å– Bronze JSON
        print(f"ğŸ“– æ­£åœ¨è¯»å– Bronze æ•°æ®: s3a://{S3_BUCKET}/{bronze_key}")
        df = spark.read.option("multiLine", "true").json(f"s3a://{S3_BUCKET}/{bronze_key}")

        # 4. æ•°æ®æ¸…æ´—ä¸è½¬æ¢
        # - å±•å¼€ web_pages
        # - ç»Ÿä¸€åˆ—å
        # - å»é‡
        silver_df = df.withColumn("web_page", array_join(col("web_pages"), "; ")) \
                      .withColumnRenamed("state-province", "state_province") \
                      .drop("web_pages") \
                      .drop_duplicates(["name", "country"])

        # 5. å†™å…¥ Iceberg Silver è¡¨ (æŒ‰å›½å®¶åˆ†åŒº)
        print("ğŸ“ æ­£åœ¨å†™å…¥ Iceberg Silver è¡¨...")
        silver_df.writeTo("local.db.universities_silver") \
            .tableProperty("format-version", "2") \
            .partitionedBy("country") \
            .createOrReplace()

        print("ğŸ‰ Silver å±‚ Iceberg è¡¨å·²å°±ç»ªï¼")
        spark.stop()

    # ç¼–æ’å·¥ä½œæµ
    raw_key = task_bronze_ingest()
    task_silver_spark_transform(raw_key)

# å®ä¾‹åŒ–
university_full_load_pipeline_dag = university_full_load_pipeline()