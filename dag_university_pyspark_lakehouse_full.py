from airflow.decorators import dag, task
from datetime import datetime
import json

# é…ç½®å¸¸é‡
S3_BUCKET = "data-platform-university-labs"
S3_CONN_ID = "aws_s3_conn"
BRONZE_KEY = "bronze/universities/all_universities_raw.json"

default_args = {
    'owner': 'airflow',
    'start_date': datetime(2023, 1, 1),
}

@dag(
    default_args=default_args,
    schedule_interval=None,  # æ‰‹åŠ¨è§¦å‘
    catchup=False,
    tags=['full_load', 'iceberg', 'silver']
)
def university_full_load_pipeline():

    @task
    def task_bronze_ingest():
        """ç¬¬ä¸€æ­¥ï¼šå…¨é‡æ‹‰å– API æ•°æ®å¹¶å­˜å…¥ S3 (Bronze å±‚)"""
        import requests
        from airflow.providers.amazon.aws.hooks.s3 import S3Hook

        print("ğŸ“¡ æ­£åœ¨ä» API è·å–å…¨é‡å¤§å­¦æ•°æ®...")
        url = "http://universities.hipolabs.com/search"
        response = requests.get(url)
        print(f"ğŸ“¡ æ­£åœ¨å°è¯•ä¸‹è½½å…¨é‡æ•°æ® (ä½¿ç”¨æµå¼æ¨¡å¼)...")
        
        # å¢åŠ  headers æ¨¡æ‹Ÿæµè§ˆå™¨ï¼Œæœ‰æ—¶èƒ½ç»•è¿‡ä»£ç†é™åˆ¶
        headers = {'User-Agent': 'Mozilla/5.0'}
        
        try:
            # å¢åŠ  timeout=60 é˜²æ­¢æ— é™æŒ‚èµ·
            response = requests.get(url, headers=headers, timeout=60, stream=True)
            response.raise_for_status() # å¦‚æœ 404 æˆ– 500 ä¼šç›´æ¥æŠ¥é”™
            
            # ä½¿ç”¨ content è€Œä¸æ˜¯ .json()ï¼Œå› ä¸º .json() åœ¨æ–­æµæ—¶ä¼šå´©
            full_content = response.content
            
            # éªŒè¯ JSON å®Œæ•´æ€§
            data = json.loads(full_content)
            print(f"âœ… ä¸‹è½½æˆåŠŸï¼è®°å½•æ•°: {len(data)}")

            s3_hook = S3Hook(aws_conn_id=S3_CONN_ID)
            s3_hook.load_string(
                string_data=json.dumps(data),
                key=BRONZE_KEY,
                bucket_name=S3_BUCKET,
                replace=True
            )
            return BRONZE_KEY

        except Exception as e:
            print(f"âŒ ä¸‹è½½å¤±è´¥: {e}")
            raise  # æŠ›å‡ºå¼‚å¸¸è®© Airflow é‡è¯•

    @task
    def task_silver_spark_transform(bronze_key):
        """ç¬¬äºŒæ­¥ï¼šä½¿ç”¨ Spark è¯»å– Bronze JSON å¹¶å†™å…¥ Iceberg (Silver å±‚)"""
        from pyspark.sql import SparkSession
        from pyspark.sql.functions import col, array_join
        from airflow.providers.amazon.aws.hooks.base_aws import AwsBaseHook

        # 1. åŠ¨æ€è·å– AWS å‡­è¯
        aws_hook = AwsBaseHook(aws_conn_id=S3_CONN_ID, client_type="s3")
        creds = aws_hook.get_credentials()

        # 2. åˆå§‹åŒ– Spark
        print("ğŸš€ å¯åŠ¨ Spark å¼•æ“...")
        spark = SparkSession.builder \
            .appName("BronzeToSilverFullLoad") \
            .config("spark.jars.packages", 
                    "org.apache.iceberg:iceberg-spark-runtime-3.4_2.12:1.3.1,"
                    "org.apache.hadoop:hadoop-aws:3.3.4") \
            .config("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions") \
            .config("spark.sql.catalog.local", "org.apache.iceberg.spark.SparkCatalog") \
            .config("spark.sql.catalog.local.type", "hadoop") \
            .config("spark.sql.catalog.local.warehouse", f"s3a://{S3_BUCKET}/iceberg-warehouse") \
            .config("spark.hadoop.fs.s3a.access.key", creds.access_key) \
            .config("spark.hadoop.fs.s3a.secret.key", creds.secret_key) \
            .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
            .config("spark.hadoop.fs.s3a.endpoint.region", "us-east-1") \
            .getOrCreate()

        # 3. è¯»å– Bronze JSON
        print(f"ğŸ“– æ­£åœ¨è¯»å– Bronze æ•°æ®: s3a://{S3_BUCKET}/{bronze_key}")
        df = spark.read.option("multiLine", "true").json(f"s3a://{S3_BUCKET}/{bronze_key}")

        # 4. æ•°æ®æ¸…æ´—ä¸è½¬æ¢
        # - å±•å¼€ web_pages
        # - ç»Ÿä¸€åˆ—å
        # - å»é‡
        silver_df = df.withColumn("web_page", array_join(col("web_pages"), "; ")) \
                      .withColumnRenamed("state-province", "state_province") \
                      .drop("web_pages") \
                      .drop_duplicates(["name", "country"])

        # 5. å†™å…¥ Iceberg Silver è¡¨ (æŒ‰å›½å®¶åˆ†åŒº)
        print("ğŸ“ æ­£åœ¨å†™å…¥ Iceberg Silver è¡¨...")
        silver_df.writeTo("local.db.universities_silver") \
            .tableProperty("format-version", "2") \
            .partitionedBy("country") \
            .createOrReplace()

        print("ğŸ‰ Silver å±‚ Iceberg è¡¨å·²å°±ç»ªï¼")
        spark.stop()

    # ç¼–æ’å·¥ä½œæµ
    raw_key = task_bronze_ingest()
    task_silver_spark_transform(raw_key)

# å®ä¾‹åŒ–
university_full_load_pipeline_dag = university_full_load_pipeline()