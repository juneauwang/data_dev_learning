from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.postgres.hooks.postgres import PostgresHook
from airflow.utils.dates import days_ago
import requests
import json
import logging
from datetime import timedelta
import random
# --- 1. å®šä¹‰å¤±è´¥æ—¶çš„é€šçŸ¥å‡½æ•° ---
def on_failure_callback(context):
    """
    å½“ä»»åŠ¡å¤±è´¥æ—¶ï¼ŒAirflow ä¼šè°ƒç”¨è¿™ä¸ªå‡½æ•°ã€‚
    ä½ å¯ä»¥åœ¨è¿™é‡Œå¯¹æ¥é’‰é’‰ã€é£ä¹¦æˆ–é‚®ä»¶ã€‚
    """
    task_id = context.get('task_instance').task_id
    err_msg = context.get('exception')
    logging.error(f"ğŸš¨ ä»»åŠ¡ {task_id} æŒ‚äº†ï¼é”™è¯¯ä¿¡æ¯: {err_msg}")

# --- 2. æŠ“å–å¹¶å­˜å‚¨æ•°æ®çš„é€»è¾‘ ---
def fetch_and_save_data():
    # æ¨¡æ‹Ÿä¸€ä¸ªå¤–éƒ¨ APIï¼šè¿™é‡Œç”¨éšæœºç”¨æˆ·æ¥å£ä½œä¸ºæ•°æ®æº
#    if random.random() < 0.5:
#        logging.info("ğŸ² è¿æ°”ä¸å¥½ï¼Œæ¨¡æ‹Ÿè§¦å‘ç½‘ç»œå¼‚å¸¸...")
#        raise ConnectionError("ğŸŒ æ¨¡æ‹Ÿç½‘ç»œè¿æ¥å¤±è´¥ï¼Airflow åº”è¯¥å‡†å¤‡é‡è¯•...")
    api_url = "http://universities.hipolabs.com/search?country=China" 
    
    response = requests.get(api_url, timeout=10)
    response.raise_for_status() # å¦‚æœçŠ¶æ€ç ä¸æ˜¯ 200ï¼Œç›´æ¥æŠ›å‡ºå¼‚å¸¸è§¦å‘é‡è¯•
    
    data = response.json()
    logging.info(f"DEBUG: æŠ“å–æˆåŠŸï¼Œå‡†å¤‡è§£æ {len(data)} æ¡è®°å½•")
#    sample_data = data[:10]
    #users = data['results']
    rows_to_insert = [
	(uni.get('name'),uni.get('alpha_two_code'),uni.get('country'))
	for uni in data
    ]     
    pg_hook = PostgresHook(postgres_conn_id='postgres_default')
    logging.info(f"DEBUG: å…ƒç»„åˆ—è¡¨æ„å»ºå®Œæˆï¼Œå‡†å¤‡å†™å…¥æ•°æ®åº“")
    # å‡†å¤‡å†™å…¥æ•°æ®åº“
# --- 3. å†™å…¥æ•°æ®åº“ ---
    pg_hook.insert_rows(
        table='raw_users',
        rows=rows_to_insert,
        target_fields=['external_id', 'username', 'email'],
        commit_every=100,
        replace=True,          # <--- å°±æ˜¯è¿™è¡Œ
        replace_index='external_id' # <--- å¿…é¡»å‘Šè¯‰å®ƒå“ªä¸ªå­—æ®µæ˜¯å†²çªåˆ¤æ–­çš„â€œå”¯ä¸€ç´¢å¼•â€
        # æ‰¹é‡å†™å…¥æ—¶çš„å†²çªå¤„ç†æ¯”è¾ƒå¤æ‚ï¼Œé€šå¸¸æˆ‘ä»¬ä¼šå…ˆå†™å…¥ä¸´æ—¶è¡¨ï¼ˆStaging Tableï¼‰
    )
    logging.info(f"âœ… æˆåŠŸæ¬è¿ {len(sample_data)} æ¡å¤§å­¦æ•°æ®ï¼")
# --- 3. å®šä¹‰ DAG ---
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'email_on_failure': False,
    'email_on_retry': False,
    # ã€æ ¸å¿ƒé…ç½®ï¼šé‡è¯•æœºåˆ¶ã€‘
    'retries': 3,                            # å¤±è´¥åé‡è¯• 3 æ¬¡
    'retry_delay': timedelta(seconds=30),     # æ¯æ¬¡é‡è¯•é—´éš” 30 ç§’
    'on_failure_callback': on_failure_callback # å¤±è´¥æ—¶è°ƒç”¨çš„å‡½æ•°
}

with DAG(
    'dag_api_ingestion_v2',
    default_args=default_args,
    description='ä»å¤–éƒ¨ API æŠ“å– JSON å¹¶å­˜å…¥ Postgres',
    schedule_interval=None,
    start_date=days_ago(1),
    catchup=False,
) as dag:

    # ä»»åŠ¡ 1ï¼šå‡†å¤‡è¡¨ç»“æ„
    prepare_table = PostgresHook(postgres_conn_id='postgres_default').run("""
        CREATE TABLE IF NOT EXISTS raw_users (
            id SERIAL PRIMARY KEY,
            external_id TEXT UNIQUE,
            username TEXT,
            email TEXT,
            ingested_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        );
    """) # æ³¨æ„ï¼šè¿™é‡Œä¸ºäº†æ¼”ç¤ºç›´æ¥å†™äº†ï¼Œå®é™…å»ºè®®ç”¨ PostgresOperator

    # ä»»åŠ¡ 2ï¼šæ‰§è¡ŒæŠ“å–
    ingest_task = PythonOperator(
        task_id='fetch_external_users',
        python_callable=fetch_and_save_data,
        # ä½ ä¹Ÿå¯ä»¥åœ¨å•ä¸ªä»»åŠ¡çº§åˆ«è¦†ç›–é‡è¯•é…ç½®
        retries=5, 
    )

    ingest_task
