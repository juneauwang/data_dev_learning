from airflow import DAG
from airflow.providers.postgres.operators.postgres import PostgresOperator
from airflow.providers.postgres.hooks.postgres import PostgresHook
from airflow.operators.python import PythonOperator
from airflow.utils.task_group import TaskGroup
from airflow.utils.dates import days_ago
import logging

# 1. å®šä¹‰ ETL å¤„ç†å‡½æ•°
def run_user_stats_etl():
    """
    ä½¿ç”¨ PostgresHook æ‰§è¡Œæ•°æ®è½¬æ¢å’Œæ¬è¿
    """
    pg_hook = PostgresHook(postgres_conn_id='postgres_default')
    
    # è¿™é‡Œçš„é€»è¾‘æ˜¯ï¼šä»ŽåŽŸå§‹æ—¥å¿—è¡¨ç»Ÿè®¡ç”¨æˆ·æ´»è·ƒåº¦ï¼Œç„¶åŽâ€œè¦†ç›–å†™å…¥â€åˆ°ç»Ÿè®¡è¡¨
    etl_sql = """
    INSERT INTO user_stats (user_id, activity_count, last_active)
    SELECT user_id, COUNT(*), MAX(created_at)
    FROM user_activity_logs
    GROUP BY user_id
    ON CONFLICT (user_id) DO UPDATE 
    SET activity_count = EXCLUDED.activity_count, 
        last_active = EXCLUDED.last_active;
    """
    
    logging.info("å¼€å§‹æ‰§è¡Œ ETL è½¬æ¢...")
    pg_hook.run(etl_sql)
    logging.info("æ•°æ®æ¬è¿æˆåŠŸï¼")

# 2. å®šä¹‰ DAG
with DAG(
    dag_id='dag_etl_v1',
    start_date=days_ago(1),
    schedule_interval=None,
    catchup=False,
    tags=['learning', 'etl']
) as dag:

    # ä»»åŠ¡ 1ï¼šåˆå§‹åŒ–è¡¨ç»“æž„ï¼ˆå¦‚æžœä¸å­˜åœ¨ï¼‰
    # åœ¨çœŸå®žå¼€å‘ä¸­ï¼Œé€šå¸¸ä¼šæœ‰ä¸“é—¨çš„ä»»åŠ¡è´Ÿè´£ DDLï¼ˆæ•°æ®å®šä¹‰è¯­è¨€ï¼‰
    prepare_tables = PostgresOperator(
        task_id='prepare_tables',
        postgres_conn_id='postgres_default',
        sql="""
        CREATE TABLE IF NOT EXISTS user_activity_logs (
            id SERIAL PRIMARY KEY,
            user_id INT,
            action TEXT,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        );
        CREATE TABLE IF NOT EXISTS user_stats (
            user_id INT PRIMARY KEY,
            activity_count INT,
            last_active TIMESTAMP
        );
        """
    )

    # ä»»åŠ¡ 2ï¼šä½¿ç”¨ TaskGroup ç»„ç»‡åˆ†æžé€»è¾‘
    with TaskGroup("analysis_processing") as analysis_group:
        
        # æ¨¡æ‹Ÿï¼šåœ¨æ¬è¿å‰å…ˆè¿›è¡Œä¸€äº›æ•°æ®æ£€æŸ¥
        check_data = PythonOperator(
            task_id='pre_check',
            python_callable=lambda: print("ðŸ” æ£€æŸ¥åŽŸå§‹æ•°æ®å®Œæ•´æ€§...")
        )

        # æ ¸å¿ƒï¼šæ‰§è¡Œ ETL æ¬è¿
        do_etl = PythonOperator(
            task_id='execute_etl',
            python_callable=run_user_stats_etl
        )

        check_data >> do_etl

    # ä»»åŠ¡ 3ï¼šæ¸…ç†æˆ–å‘é€é€šçŸ¥
    post_cleanup = PostgresOperator(
        task_id='post_cleanup',
        postgres_conn_id='postgres_default',
        sql="-- è¿™é‡Œå¯ä»¥å†™ä¸€äº›æ¸…ç†æ—§ä¸´æ—¶æ•°æ®çš„ SQL \n SELECT 1;"
    )

    # 3. å®šä¹‰ä»»åŠ¡ä¾èµ–
    prepare_tables >> analysis_group >> post_cleanup
