from airflow.decorators import dag, task
from datetime import datetime
import json

# é…ç½®å¸¸é‡ - ä»¥åä½ å°±æ˜¯é‡åŒ–æ•°æ®å·¥ç¨‹å¸ˆäº†
S3_BUCKET = "data-platform-university-labs"
S3_CONN_ID = "aws_s3_conn"
BRONZE_KEY = "bronze/crypto/markets_top100.json"
# ä½¿ç”¨ CoinGecko API è·å–å¸‚å€¼å‰ 100 çš„å¸ç§
API_URL = "https://api.coingecko.com/api/v3/coins/markets?vs_currency=usd&order=market_cap_desc&per_page=100&page=1"

default_args = {
    'owner': 'airflow',
    'start_date': datetime(2023, 1, 1),
}

@dag(
    default_args=default_args,
    schedule_interval="@hourly",  # æ—¢ç„¶æ˜¯é‡åŒ–ï¼Œæˆ‘ä»¬å¯ä»¥æ¯å°æ—¶è·‘ä¸€æ¬¡
    catchup=False,
    tags=['crypto', 'iceberg', 'quant']
)
def crypto_lakehouse_pipeline():

    @task(retries=3, retry_delay=30)
    def task_bronze_ingest_crypto():
        """ç¬¬ä¸€æ­¥ï¼šå…¨é‡æ‹‰å– CoinGecko æ•°æ®å¹¶å­˜å…¥ S3 (Bronze å±‚)"""
        import requests
        from airflow.providers.amazon.aws.hooks.s3 import S3Hook

        print(f"ğŸ“¡ æ­£åœ¨ä» CoinGecko è·å–è¡Œæƒ…æ•°æ®...")
        headers = {'User-Agent': 'Mozilla/5.0'}
        
        try:
            # ä½¿ç”¨æˆ‘ä»¬å®šä¹‰çš„ API_URL
            response = requests.get(API_URL, headers=headers, timeout=60)
            response.raise_for_status()
            
            data = response.json()
            print(f"âœ… ä¸‹è½½æˆåŠŸï¼è·å–åˆ° {len(data)} ä¸ªå¸ç§è¡Œæƒ…")

            s3_hook = S3Hook(aws_conn_id=S3_CONN_ID)
            s3_hook.load_string(
                string_data=json.dumps(data),
                key=BRONZE_KEY,
                bucket_name=S3_BUCKET,
                replace=True
            )
            return BRONZE_KEY

        except Exception as e:
            print(f"âŒ Crypto æ•°æ®é‡‡é›†å¤±è´¥: {e}")
            raise 

    @task
    def task_silver_spark_quant_transform(bronze_key):
        """ç¬¬äºŒæ­¥ï¼šé‡åŒ–æ¸…æ´—ï¼Œå­˜å…¥ Iceberg Silver å±‚"""
        from pyspark.sql import SparkSession
        from pyspark.sql.functions import col, to_timestamp
        from airflow.providers.amazon.aws.hooks.base_aws import AwsBaseHook

        # 1. è·å– AWS å‡­è¯
        aws_hook = AwsBaseHook(aws_conn_id=S3_CONN_ID, client_type="s3")
        creds = aws_hook.get_credentials()

        # 2. åˆå§‹åŒ– Spark
        spark = SparkSession.builder \
            .appName("CryptoBronzeToSilver") \
            .config("spark.jars.packages", 
                    "org.apache.iceberg:iceberg-spark-runtime-3.4_2.12:1.3.1,"
                    "org.apache.hadoop:hadoop-aws:3.3.4") \
            .config("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions") \
            .config("spark.sql.catalog.local", "org.apache.iceberg.spark.SparkCatalog") \
            .config("spark.sql.catalog.local.type", "hadoop") \
            .config("spark.sql.catalog.local.warehouse", f"s3a://{S3_BUCKET}/iceberg-warehouse") \
            .config("spark.hadoop.fs.s3a.access.key", creds.access_key) \
            .config("spark.hadoop.fs.s3a.secret.key", creds.secret_key) \
            .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
            .config("spark.hadoop.fs.s3a.endpoint.region", "us-east-1") \
            .getOrCreate()

        # 3. è¯»å– Bronze JSON
        print(f"ğŸ“– æ­£åœ¨è¯»å– Crypto åŸå§‹æ•°æ®...")
        df = spark.read.option("multiLine", "true").json(f"s3a://{S3_BUCKET}/{bronze_key}")
        
        # 4. é‡åŒ–å­—æ®µæ¸…æ´—
        silver_df = df.select(
            col("id"),
            col("symbol"),
            col("name"),
            col("current_price").cast("double"),
            col("market_cap").cast("long"),
            col("total_volume").cast("long"),
            col("price_change_percentage_24h").alias("pct_change_24h"),
            to_timestamp(col("last_updated")).alias("updated_at")
        ).drop_duplicates(["id", "updated_at"])

        # 5. å†™å…¥ Iceberg (æŒ‰ id åˆ†åŒº)
        print("ğŸ“ æ­£åœ¨æ›´æ–° Iceberg Silver è¡¨ (crypto_silver)...")
        silver_df.writeTo("local.db.crypto_silver") \
            .tableProperty("format-version", "2") \
            .partitionedBy("id") \
            .createOrReplace()

        print("ğŸ‰ åŠ å¯†è´§å¸ Silver å±‚æ•°æ®è½¬æ¢å®Œæˆï¼")
        spark.stop()

    # æ‰§è¡Œæµç¨‹
    bronze_file = task_bronze_ingest_crypto()
    task_silver_spark_quant_transform(bronze_file)

# å®ä¾‹åŒ–
crypto_dag = crypto_lakehouse_pipeline()