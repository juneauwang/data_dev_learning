from airflow.decorators import dag, task
from datetime import datetime
import json

# é…ç½®å¸¸é‡ - ä»¥åä½ å°±æ˜¯é‡åŒ–æ•°æ®å·¥ç¨‹å¸ˆäº†
S3_BUCKET = "data-platform-university-labs"
S3_CONN_ID = "aws_s3_conn"
BRONZE_KEY = "bronze/crypto/markets_top100.json"
# ä½¿ç”¨ CoinGecko API è·å–å¸‚å€¼å‰ 100 çš„å¸ç§
API_URL = "https://api.coingecko.com/api/v3/coins/markets?vs_currency=usd&order=market_cap_desc&per_page=100&page=1"

default_args = {
    'owner': 'airflow',
    'start_date': datetime(2023, 1, 1),
}

@dag(
    default_args=default_args,
    schedule_interval="@hourly",  # æ—¢ç„¶æ˜¯é‡åŒ–ï¼Œæˆ‘ä»¬å¯ä»¥æ¯å°æ—¶è·‘ä¸€æ¬¡
    catchup=False,
    tags=['crypto', 'iceberg', 'quant']
)
def crypto_lakehouse_pipeline():

    @task(retries=3, retry_delay=30)
    def task_bronze_ingest_crypto():
        """ç¬¬ä¸€æ­¥ï¼šå…¨é‡æ‹‰å– CoinGecko æ•°æ®å¹¶å­˜å…¥ S3 (Bronze å±‚)"""
        import requests
        from airflow.providers.amazon.aws.hooks.s3 import S3Hook

        print(f"ğŸ“¡ æ­£åœ¨ä» CoinGecko è·å–è¡Œæƒ…æ•°æ®...")
        headers = {'User-Agent': 'Mozilla/5.0'}
        
        try:
            # ä½¿ç”¨æˆ‘ä»¬å®šä¹‰çš„ API_URL
            response = requests.get(API_URL, headers=headers, timeout=60)
            response.raise_for_status()
            
            data = response.json()
            print(f"âœ… ä¸‹è½½æˆåŠŸï¼è·å–åˆ° {len(data)} ä¸ªå¸ç§è¡Œæƒ…")

            s3_hook = S3Hook(aws_conn_id=S3_CONN_ID)
            s3_hook.load_string(
                string_data=json.dumps(data),
                key=BRONZE_KEY,
                bucket_name=S3_BUCKET,
                replace=True
            )
            return BRONZE_KEY

        except Exception as e:
            print(f"âŒ Crypto æ•°æ®é‡‡é›†å¤±è´¥: {e}")
            raise 

    @task
    def task_silver_spark_quant_transform(bronze_key):
        """ç¬¬äºŒæ­¥ï¼šé‡åŒ–æ¸…æ´—ï¼Œå­˜å…¥ Iceberg Silver å±‚"""
        from pyspark.sql import SparkSession
        from pyspark.sql.functions import col, to_timestamp
        from airflow.providers.amazon.aws.hooks.base_aws import AwsBaseHook

        # 1. è·å– AWS å‡­è¯
        aws_hook = AwsBaseHook(aws_conn_id=S3_CONN_ID, client_type="s3")
        creds = aws_hook.get_credentials()

        # 2. åˆå§‹åŒ– Spark
        spark = SparkSession.builder \
            .appName("CryptoBronzeToSilver") \
            .config("spark.jars.packages", 
                    "org.apache.iceberg:iceberg-spark-runtime-3.4_2.12:1.3.1,"
                    "org.apache.hadoop:hadoop-aws:3.3.4") \
            .config("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions") \
            .config("spark.sql.catalog.local", "org.apache.iceberg.spark.SparkCatalog") \
            .config("spark.sql.catalog.local.type", "hadoop") \
            .config("spark.sql.catalog.local.warehouse", f"s3a://{S3_BUCKET}/iceberg-warehouse") \
            .config("spark.hadoop.fs.s3a.access.key", creds.access_key) \
            .config("spark.hadoop.fs.s3a.secret.key", creds.secret_key) \
            .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
            .config("spark.hadoop.fs.s3a.endpoint.region", "us-east-1") \
            .getOrCreate()

        # 3. è¯»å– Bronze JSON
        print(f"ğŸ“– æ­£åœ¨è¯»å– Crypto åŸå§‹æ•°æ®...")
        df = spark.read.option("multiLine", "true").json(f"s3a://{S3_BUCKET}/{bronze_key}")
        
        # 4. é‡åŒ–å­—æ®µæ¸…æ´—
        silver_df = df.select(
            col("id"),
            col("symbol"),
            col("name"),
            col("current_price").cast("double"),
            col("market_cap").cast("long"),
            col("total_volume").cast("long"),
            col("price_change_percentage_24h").alias("pct_change_24h"),
            to_timestamp(col("last_updated")).alias("updated_at")
        ).drop_duplicates(["id", "updated_at"])

        # 5. å†™å…¥ Iceberg (æŒ‰ id åˆ†åŒº)
        print("ğŸ“ æ­£åœ¨æ›´æ–° Iceberg Silver è¡¨ (crypto_silver)...")
        silver_df.writeTo("local.db.crypto_silver") \
            .tableProperty("format-version", "2") \
            .partitionedBy("id") \
            .createOrReplace()

        print("ğŸ‰ åŠ å¯†è´§å¸ Silver å±‚æ•°æ®è½¬æ¢å®Œæˆï¼")
        spark.stop()
        return "Silver Table Updated"
        
    @task
    def task_gold_spark_analysis(upstream_status):
        from pyspark.sql import SparkSession
        from pyspark.sql import functions as F
        print(f"ğŸš€ æ¥æ”¶åˆ°ä¸Šæ¸¸çŠ¶æ€: {upstream_status}ï¼Œå¼€å§‹ Gold å±‚è®¡ç®—...")
        # ä¿æŒé…ç½®ä¸€è‡´æ€§
        spark = SparkSession.builder \
            .appName("CryptoGoldQuant") \
            .config("spark.sql.catalog.local", "org.apache.iceberg.spark.SparkCatalog") \
            .config("spark.sql.catalog.local.type", "hadoop") \
            .config("spark.sql.catalog.local.warehouse", "s3a://data-platform-university-labs/iceberg-warehouse") \
            .getOrCreate()

        # 1. åŠ è½½ Silver è¡¨
        silver_df = spark.table("local.db.crypto_silver")

        # 2. è®¡ç®—é‡åŒ–æŒ‡æ ‡
        # å…ˆè®¡ç®—å…¨å±€æ€»å¸‚å€¼ç”¨äºæƒé‡è®¡ç®—
        total_market_cap = silver_df.select(F.sum("market_cap")).collect()[0][0]

        gold_df = silver_df.withColumn(
            "market_cap_weight", 
            F.round((F.col("market_cap") / total_market_cap) * 100, 4)
        ).withColumn(
            "volatility_tier",
            F.when(F.abs("price_change_percentage_24h") >= 10, "Extreme")
            .when(F.abs("price_change_percentage_24h") >= 5, "High")
            .otherwise("Stable")
        ).withColumn(
            "is_top_dominance", 
            F.col("market_cap_weight") > 1.0  # æƒé‡è¶…è¿‡ 1% çš„å¸ç§
        ).select(
            "id", "symbol", "current_price", 
            "market_cap_weight", "volatility_tier", "is_top_dominance",
            F.current_timestamp().alias("analysis_at")
        )

        # 3. å†™å…¥ Gold è¡¨ (Iceberg æ ¼å¼)
        # ä½¿ç”¨ createOrReplace ä»¥ä¾¿æˆ‘ä»¬åå¤è°ƒè¯•
        gold_df.writeTo("local.db.crypto_gold_metrics").createOrReplace()
        
        print("âœ¨ Gold é‡åŒ–è¡¨å·²ç”Ÿæˆï¼")
        gold_df.show(10)
        spark.stop()
        

    # æ‰§è¡Œæµç¨‹
    bronze_file = task_bronze_ingest_crypto()
    silver_status = task_silver_spark_quant_transform(bronze_file)
    task_gold_spark_analysis(silver_status)

# å®ä¾‹åŒ–
crypto_dag = crypto_lakehouse_pipeline()