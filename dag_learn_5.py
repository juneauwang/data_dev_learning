from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.utils.dates import days_ago
from airflow.sensors.sql import SqlSensor
# å®šç¾©æ•¸æ“šè™•ç†é‚è¼¯
def process_new_data():
    print("ðŸŽ¯ å“¨å…µå ±å‘Šï¼šæª¢æ¸¬åˆ°æ–°æ•¸æ“šå·²å…¥åº«ï¼æ­£åœ¨å•Ÿå‹•ä¸‹æ¸¸åˆ†æžç¨‹åº...")

with DAG(
    'postgres_sensor_v1',
    start_date=days_ago(1),
    schedule_interval=None,
    catchup=False
) as dag:

    # 1. å“¨å…µä»»å‹™ï¼šæ¯éš” 30 ç§’æª¢æŸ¥ä¸€æ¬¡æ•¸æ“šåº«
    wait_for_data = SqlSensor(
        task_id='wait_for_postgres_data',
        conn_id='postgres_default',
        # åªè¦ count > 0ï¼ŒSensor å°±æœƒè®Šç¶ 
        sql="SELECT COUNT(*) FROM user_activity_logs WHERE user_id = 999;",
        poke_interval=30,      # æª¢æŸ¥é »çŽ‡ï¼ˆç§’ï¼‰
        timeout=600,           # 10 åˆ†é˜å¾Œè‹¥é‚„æ²’æ•¸æ“šå°±è¶…æ™‚å ±éŒ¯
        mode='poke'            # æŒçºŒä½”ç”¨ Worker ç­‰å¾…ï¼ˆåˆå­¸è€…å»ºè­°å…ˆç”¨é€™å€‹ï¼‰
    )

    # 2. ä¸‹æ¸¸è™•ç†ä»»å‹™
    do_analysis = PythonOperator(
        task_id='run_analysis',
        python_callable=process_new_data
    )

    wait_for_data >> do_analysis
